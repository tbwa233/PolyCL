import torch
import wandb
from torch import nn
from torch.nn import functional as F
import logging
import numpy as np

#Parses through the config file and returns a dictionary specifying hyperparameters and values
def ParseConfig(configFile, vars):
    file = open(configFile).readlines()
    for line in file:
        if line[0] == "#" or line[0] == "\n":
            continue

        key = line.split('=')[0]
        val = line.split('=')[1]

        if key in vars:
            vars[key] = float(val)

#Gets the segmentation masks generated by net for the iter set of slices
def getMasks(net, iter, device=None):
    net.eval()

    segmentationMask = []

    with torch.no_grad():
        for i, (X, y1, y2) in enumerate(iter):
            X = X.to(device)
            y1 = y1.to(device)
            y2 = y2.to(device)

            segment, _ = net(X)

            for slice in segment:
                segmentationMask.append(torch.round(slice).squeeze(0).tolist())

    return segmentationMask

#Primary evluation function for any loss functions
#Works for segmentation and classification eval
def evaluate(net, testIter, lossFuncs, device=None, encoder=False):
    net.eval()

    #lossFuncs contains 2 lists of eval functions, first corresponding to segmentation and second corresponding to classification

    #Creates 0 lists for the sums of metrics and lengths for calculating average losses
    #A list of lengths is necessary for loss functions that are sometimes undefined like Hausdorff distance
    metric = [
        [0 for _ in range(len(lossFuncs[0]))],
        [0 for _ in range(len(lossFuncs[1]))]
    ]

    lengths = [
        [0 for _ in range(len(lossFuncs[0]))],
        [0 for _ in range(len(lossFuncs[1]))]
    ]

    with torch.no_grad():
        for i, (X, y1, y2) in enumerate(testIter):
            X = X.to(device)
            y1 = y1.to(device)
            y2 = y2.to(device)

            yhat = net(X)

            #Loops through all segmentation loss functions
            for i, segmentLoss in enumerate(lossFuncs[0]):
                l = segmentLoss(yhat[0] if isinstance(yhat, tuple) else yhat, y1)
                
                #If the loss contains multiple values (Hausdorff distance for multiple classes) and the metric array where the values should be stored hasn't been defined yet, define it as an array
                if isinstance(l, np.ndarray) and not isinstance(metric[0][i], np.ndarray):
                    metric[0][i] = np.array([0 for _ in range(y1.size(dim=1))], dtype=np.float64)
                    lengths[0][i] = np.array([0 for _ in range(y1.size(dim=1))], dtype=np.float64)

                #Skips any values that are negative, adds to the metric and length totals
                if isinstance(l, np.ndarray) or l >= 0:
                    if isinstance(l, np.ndarray):
                        for q, el in enumerate(l):
                            if el > 0:
                                metric[0][i][q] += el
                                lengths[0][i][q] += 1
                    else:
                        metric[0][i] += l
                        lengths[0][i] += 1

            #Loops through all classification loss functions
            for i, classLoss in enumerate(lossFuncs[1]):
                l = classLoss(yhat[0], y2) if encoder else (classLoss(yhat[1] if isinstance(yhat, tuple) else yhat, y2))
                
                #Skips any negative values and adds to the metric and length totals
                if l >= 0:
                    metric[1][i] += l
                    lengths[1][i] += 1

    #Divides all values to calculate averages
    for i in range(len(metric[0])):
        if isinstance(metric[0][i], np.ndarray):
            metric[0][i] = np.divide(metric[0][i], lengths[0][i])
        else:
            metric[0][i] /= lengths[0][i] if lengths[0][i] > 0 else 1

    for i in range(len(metric[1])):
        metric[1][i] /= lengths[1][i] if lengths[1][i] > 0 else 1

    return metric

#Primary training function
def train(net: nn.Module, lossFuncs, weights, trainIter, testIter, numEpochs, startEpoch, learnRate, device: torch.device, startDim, epochsToDouble, modelFileName, epochsToSave, 
          useWandB=False, cosineAnnealing=True, restartEpochs=-1, progressive=False, encoder=False, unfreezeEpoch=-1):
    print(f"Training on {device}")
    
    #Checks that the length of the loss function list is 2
    #It should contain 2 lists for segmentation and classification loss
    if len(weights) != 2 or len(lossFuncs) != 2:
        logging.error("Length of weights or loss functions list != 2")
        return
    
    #Checks that the lengths of the weights and functions are equal
    if len(weights[0]) != len(lossFuncs[0]) or len(weights[1]) != len(lossFuncs[1]):
        logging.error("Length of weights and loss functions is not equal")
        return

    net.to(device)
    optimizer = torch.optim.Adam(net.parameters(), lr=learnRate)

    #Setting restartEpochs to a negative will use no warm restarts, otherwise will use warm restarts 
    if cosineAnnealing:
        if restartEpochs <= 0:
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=numEpochs)
        else:
            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, restartEpochs, T_mult=1)

    numBatches = len(trainIter)
    bestValLoss = float('inf')

    classPerf = 0
    segmentPerf = 0

    currDim = startDim
    for epoch in range(startEpoch, numEpochs):
        net.train()
        
        loss = 0

        #Unfreezes the encoder if it started frozen and needed to be unfrozen at a certain time
        if epoch == unfreezeEpoch:
            net.freezeEncoder(state=True)

        for i, (X, y1, y2) in enumerate(trainIter):
            optimizer.zero_grad()
            y1 = y1.to(device)
            y2 = y2.to(device)

            if progressive != 0:
                #If using progressive learning, downsamples image to the current dimension
                X = F.interpolate(X, size=int(currDim))

            X = X.to(device)
            
            #Gives the input to the network, also specifies whether the classification/segmentation performance has crossed the threshold
            yhat = net(X, classPerf >= net.classThreshold, segmentPerf >= net.segmentThreshold)

            #Sets the output based on whether the model being trained is full UNet or an encoder
            segmentYHat = yhat[0] if not encoder else None
            classYHat = yhat[1] if not encoder else yhat[0]

            #Gets total loss by looping through all segmentation and classification loss functions
            #Multiplies all values by their corresponding weights
            l = 0
            for i, func in enumerate(lossFuncs[0]):
                if weights[0][i] > 0:
                    l += weights[0][i] * func(segmentYHat, y1)

            for i, func in enumerate(lossFuncs[1]):
                if weights[1][i] > 0:
                    l += weights[1][i] * func(classYHat, y2)

            #print(f"Loss: {l.item()} Predictions: {yhat.tolist()} Labels: {y.tolist()}")

            l.backward()
            optimizer.step()

            if cosineAnnealing:
                scheduler.step(epoch + i / numBatches)

            loss += l

        #Progressive learning
        if (epoch + 1) % epochsToDouble == 0 and progressive == 1:
            currDim *= 2
        #Reverse progressive learning
        elif (epoch + 1) % epochsToDouble == 0 and progressive == 2:
            currDim /= 2

        #Checkpoints model
        if (epoch + 1) % epochsToSave == 0:
            torch.save(net.state_dict(), modelFileName + "Epoch" + str(epoch))

        #Evaluates the model on the validation set, uses those values to create a log string for logging the performance at each epoch
        valLosses = evaluate(net, testIter, lossFuncs, device=device, encoder=encoder)

        validationLoss = 0

        logStr = ""

        if net.multiTask:
            segmentPerf = valLosses[0][0]
            classPerf = valLosses[1][0]

        for i, arr in enumerate(valLosses):
            for j, val in enumerate(arr):
                validationLoss += weights[i][j] * val
                logStr += (lossFuncs[i][j].__name__ if str(type(lossFuncs[i][j])) == "<class 'function'>" else type(lossFuncs[i][j]).__name__) + ": " + str(val) + " "

        #Overwrites previous best model based on validation performance
        if validationLoss < bestValLoss:
            bestValLoss = validationLoss
            torch.save(net.state_dict(), modelFileName + "BestLoss")

        print(f"Epoch {epoch}:\nTrain Loss: {loss / numBatches} Validation Loss: {validationLoss} " + logStr)

        #Externally logs epoch info to WandB
        if useWandB:
            wandb.log({"Train Loss": loss / numBatches,
                    "Validation Loss": validationLoss
                    })

#Modified version of above training function but much more specialized for PolyCL contrastive pre-training
def contrastiveTrain(net: nn.Module, distFunc, trainIter, numEpochs, startEpoch, learnRate, device: torch.device, modelFileName, epochsToSave, useWandB=False, cosineAnnealing=True, restartEpochs=-1):
    print(f"Training on {device}")

    net.to(device)
    optimizer = torch.optim.Adam(net.parameters(), lr=learnRate)

    #Setting restartEpochs to a negative will use no warm restarts, otherwise will use warm restarts 
    if cosineAnnealing:
        if restartEpochs <= 0:
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=numEpochs)
        else:
            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, restartEpochs, T_mult=1)

    numBatches = len(trainIter)

    for epoch in range(startEpoch, numEpochs):
        net.train()
        loss = 0

        for i, (main, pos, neg) in enumerate(trainIter):
            optimizer.zero_grad()
            
            main = main.to(device)
            pos = pos.to(device)
            neg = neg.to(device)
            
            #Gets the projected representations of each slice (anchor, positive, negative)
            mainRep, _, _, _, _, _ = net(main)
            negRep, _, _, _, _, _ = net(neg)
            posRep, _, _, _, _, _ = net(pos)

            l = distFunc(mainRep, posRep, negRep)
            l = torch.mean(l)

            l.backward()
            optimizer.step()

            if cosineAnnealing:
                scheduler.step(epoch + i / numBatches)

            loss += l

        #Checkpoints model
        if (epoch + 1) % epochsToSave == 0:
            torch.save(net.state_dict(), modelFileName + "Epoch" + str(epoch))

        print(f"Epoch {epoch}:\nTrain Loss: {loss / numBatches}")

        #Externally logs epoch info to WandB
        if useWandB:
            wandb.log({"Train Loss": loss / numBatches})

#Very similar function to the above contrastive training, but specifically made for SimCLR training
#The only real change is that a negative example is not considered as above
#In a future update, can probably combine both contrastive training functions
def simCLRTrain(net: nn.Module, distFunc, trainIter, numEpochs, startEpoch, learnRate, device: torch.device, modelFileName, epochsToSave, useWandB=False, cosineAnnealing=True, restartEpochs=-1):
    print(f"Training on {device}")

    net.to(device)
    optimizer = torch.optim.Adam(net.parameters(), lr=learnRate)

    #Setting restartEpochs to a negative will use no warm restarts, otherwise will use warm restarts 
    if cosineAnnealing:
        if restartEpochs <= 0:
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=numEpochs)
        else:
            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, restartEpochs, T_mult=1)

    numBatches = len(trainIter)

    for epoch in range(startEpoch, numEpochs):
        net.train()
        
        loss = 0

        for i, (main, pos) in enumerate(trainIter):
            optimizer.zero_grad()
            
            main = main.to(device)
            pos = pos.to(device)
            
            mainRep, _, _, _, _, _ = net(main)
            posRep, _, _, _, _, _ = net(pos)
            
            l = distFunc(mainRep, posRep)

            l.backward()
            optimizer.step()

            if cosineAnnealing:
                scheduler.step(epoch + i / numBatches)

            loss += l

        #Checkpoints model
        if (epoch + 1) % epochsToSave == 0:
            torch.save(net.state_dict(), modelFileName + "Epoch" + str(epoch))

        print(f"Epoch {epoch}:\nTrain Loss: {loss / numBatches}")

        #Externally logs epoch info to WandB
        if useWandB:
            wandb.log({"Train Loss": loss / numBatches})